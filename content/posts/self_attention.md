+++
title = "Self-attention"
author = ["Jethro Kuan"]
lastmod = 2020-10-27T17:21:14+08:00
slug = "self_attention"
draft = false
+++

The self-attention mechanism is a defining characteristic of Transformer models.
They can be viewed as a graph-like inductive bias that connects all tokens in a
sequence with a relevance-based pooling operation.
