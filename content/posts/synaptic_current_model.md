+++
title = "Synaptic Current Model"
author = ["Jethro Kuan"]
draft = false
+++

Synaptic currents are generated by synaptic currents triggered by
arrival of presynaptic spikes \\(S\_{j}^{(l)}(t)\\). Spike trains
\\(S\_{j}^{(l)}(t)\\) are denoted as a sum of Dirac delta functions
\\(S\_{j}^{(l)}(t)=\sum\_{s \in C\_{j}^{(l)}} \delta(t-s)\\), where \\(s\\) runs
over the firing times \\(C_j^{(l)}\\) of neuron \\(j\\) in layer \\(l\\).

A good first-order approximation of the synaptic current is one of
exponential decay. Synaptic currents are also assumed to sum linearly.

\begin{equation} \label{eq:scm}
\frac{\mathrm{d} I\_{i}^{(l)}}{\mathrm{d} t}=-\underbrace{\frac{I\_{i}^{(l)}(t)}{\tau\_{\mathrm{syn}}}}\_{\mathrm{exp} . \text { decay }}+\underbrace{\sum\_{j} W\_{i j}^{(l)} S\_{j}^{(l-1)}(t)}\_{\text {feed-forward }}+\underbrace{\sum\_{j} V\_{i j}^{(l)} S\_{j}^{(l)}(t)}\_{\text {recurrent }}
\end{equation}

A single LIF neuron can be simulated with 2 linear differential
equations whose initial conditions change instantaneously when a spike
occurs. Combining the reset term with the equation for the
[Leaky Integrate-And-Fire]({{<relref "leaky_integrate_and_fire.md" >}}) model, we get:

\begin{equation} \label{eq:lif_with_reset}
\frac{\mathrm{d} U\_{i}^{(l)}}{\mathrm{d} t}=-\frac{1}{\tau\_{\mathrm{mem}}}\left(\left(U\_{i}^{(l)}-U\_{\mathrm{rest}}\right)+R I\_{i}^{(l)}\right)+S\_{i}^{(l)}(t)\left(U\_{\mathrm{rest}}-\vartheta\right)
\end{equation}

The solutions to Equations [eq:scm](#eq:scm) and [eq:lif_with_reset](#eq:lif_with_reset) are
approximated numerically by discretizing time, and expressing the
output spike-train \\(S\_{i}^{(l)}(n)\\) of neuron \\(i\\) in layer \\(l\\) at
time-step \\(n\\) as a non-linear function of the membrane voltage
\\(S_i^{(l)}(n) \equiv \Theta(U_i^{(l)(n)} - \theta)\\) where \\(\theta\\) is
the Heaviside step function, and \\(\theta\\) is the firing threshold.

Setting \\(U\_{\text{rest}} = 0\\), \\(R=1\\), \\(\theta=1\\), and using some small
simulation time step \\(\delta t > 0\\), we get:

\begin{equation}
I\_{i}^{(l)}[n+1]=\alpha I\_{i}^{(l)}[n]+\sum\_{j} W\_{i j}^{(l)} S\_{j}^{(l)}[n]+\sum\_{j} V\_{i j}^{(l)} S\_{j}^{(l)}[n]
\end{equation}

with decay strength \\(\alpha \equiv \exp
\left(-\frac{\Delta\_{t}}{\tau\_{\mathrm{syn}}}\right)\\). Equation
[eq:lif_with_reset](#eq:lif_with_reset) can then be expressed as:

\begin{equation}
U\_{i}^{(l)}[n+1]=\beta U\_{i}^{(l)}[n]+I\_{i}^{(l)}[n]-S\_{i}^{(l)}[n]
\end{equation}

with \\(\beta \equiv \exp
\left(-\frac{\Delta\_{t}}{\tau\_{\operatorname{mem}}}\right)\\). These two
equations characterise the dynamics of a RNN. Specifically, the state
of neuron \\(i\\) is given by the instantaneous synaptic currents \\(I_i\\)
and the membrane voltage \\(U_i\\).

## References {#references}

- ([Neftci, Mostafa, and Zenke, n.d.](#orgdb54489))

## Bibliography {#bibliography}

<a id="orgdb54489"></a>Neftci, Emre O., Hesham Mostafa, and Friedemann Zenke. n.d. “Surrogate Gradient Learning in Spiking Neural Networks.” <http://arxiv.org/abs/1901.09948v2>.
