:PROPERTIES:
:ID:       4b8e5eef-7de5-4201-959f-519367b88535
:END:
#+title: Speeding Up the Training of Deep Neural Networks
#+bibliography: biblio.bib

Distributed training architectures rely on two concepts: [[id:9ecea56e-328e-4676-9235-c768c4256757][all-reduce]] or a
[[id:c5cb05b8-35da-4bad-8fa4-6993f34272a1][parameter server]].


* BytePS [cite:@jiangUnifiedArchitectureAccelerating2020]
:PROPERTIES:
:CODE:     https://github.com/bytedance/byteps
:END:

BytePS provides a unifying framework for [[id:9ecea56e-328e-4676-9235-c768c4256757][All-reduce]] and [[id:c5cb05b8-35da-4bad-8fa4-6993f34272a1][parameter server]]
architectures, showing communication optimality. Intra-machine communication is
optimized. It also proposes a "Summation Service", which accelerates DNN
training by running gradient summation on CPUs, while performing parameter
updates on GPUs.
